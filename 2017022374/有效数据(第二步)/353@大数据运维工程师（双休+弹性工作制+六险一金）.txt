大数据运维工程师（双休+弹性工作制+六险一金）
岗位职责：
1、负责高并发、大存储和实时流的Hadoop/spark大数据平台规划，运维，监控和优化工作；
2、保证Hadoop/spark平台各核心服务运行的稳定、高效；
3、对Hadoop/spark平台运维不断优化，提升数据产品的质量和响应速度；
4、开发各种Hadoop大数据自动化运维与监控工具；
5、研究大数据前沿技术，改进现有系统的服务和运维架构，提升系统可靠性和可运维性。

岗位要求：
1、大专或以上学历，计算机科学与技术类专业；
2、 2年或以上的实际网络和服务器运维工作经验 ；
3、熟悉HDFS/HIVE/Spark/Flume/Kafka等大数据生态的基础运维；
4、熟悉大数据周边相关的数据库系统，mysql和 mongodb/redis等；
5、具有大数据项目：包括不限于hadoop、hive、kafka、hbase、spark、Kudu、Impala等大数据生态的平台搭建、监控、运维、调优工作经验；
6、具有生产环境hadoop集群trouble shooting 、hadoop版本升级管理、技术支持，数据备份工作经验；
7、熟悉Linux的维护和管理，特别是ubuntu server，熟悉bat及Shell脚本开发，能看懂Python/Scala优先；
8、熟悉数据库mysql/oracle 安装、备份以及性能优化的优先；
9、熟悉系统组网、网络安全体系及网络监控等工具；
10、良好的团队合作精神、沟通、协调能力，责任心强。
