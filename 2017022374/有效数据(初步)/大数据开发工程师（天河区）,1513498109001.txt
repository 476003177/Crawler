大数据开发工程师（天河区）
1. 负责大数据平台的、Spark SQL和Spark Streaming技术研发、性能优化、问题诊断
2. 负责Hadoop相关研发、性能优化、问题诊断
要求
1.两年以上大数据平台设计和开发经验，具备优秀的编程能力和良好的开发习惯。
2.具备独立架构设计、内核开发、测试与运维的能力，有过大规模系统设计和工程实现的经验。
3.熟悉Spark、Spark SQL和Spark Streaming内核原理；了解Hadoop生态组件相关技术，例如Hadoop、Hive、HBase、Storm等。
4.精通Java、Scala语言，熟悉Linux 操作系统，熟练使用Python、Shell脚本语言。
5.具有认真的技术态度，良好的团队沟通和协作能力。
